{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-26/blob/main/M3_AST_26_Automatic_Speech_Recognition_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 26 : Implementation of Automatic Speech Recognition (ASR) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand SpeechBrain - A Pytorch based speech toolkit\n",
        "* use pretrained Encoder-Decoder ASR model from SpeechBrain\n",
        "* evaluate the different ASR models\n",
        "* understand the performance of ASR models using Word Error Rate (WER) and Character Error Rate (CER)\n",
        "* Finetune the EncoderDecoderASR model and evaluate on the commonvoice dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdonhbE2xWt4"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned. Various databases with text from audiobooks, conversations, and talks have been recorded.\n",
        "\n",
        "The datasets can be downloaded from the following [link](https://www.openslr.org/12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhdMHT-qRiqs"
      },
      "source": [
        "#### About DataSet:\n",
        "\n",
        "OpenSLR(Open speech and language resources) has 93 SLRs in the domain of software, audio, music, speech, and text dataset open for download. The Librispeech dataset is SLR12 which is the audio recording of reading English speech. The file format of data is in the form of FLAC(Free Lossless Audio Codec) without any loss in quality or loss of any original audio data.\n",
        "\n",
        "The audio is in English. There are two configurations: `clean` and `other`. The speakers in the corpus were ranked according to the WER (Word Error Rate) of the transcripts of a model trained on a different dataset, and were divided roughly in the middle, with the lower-WER speakers designated as \"clean\" and the higher WER speakers designated as \"other\".\n",
        "\n",
        "The **Common Voice Corpus** is Mozilla’s initiative to create a free database for speech recognition software. The project is supported by volunteers who record sample sentences with a microphone and review recordings of other users. The website clearly informs the volunteers of the purpose of recordings. The text is derived from different open-source text sources, including user-submitted blog posts, old books, movies and other public speech corpora. As of May 2021, the dataset contains 7.3k hours of transcribed and validated speech in 60 languages. Common Voice is challenging due to significant accented speech, hesitations, presence of foreign words, noise, reverberation, and other recording artifacts. it's in the format of .mp3\n",
        "\n",
        "**Whisper-spire** is the data collected from the SPIRE (Signal Processing Interpretation and Representation Laboratory) from Indian Institute of Science, Bangalore. The data is in the format of .wav files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Onl8YF9mR9"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_26_Automatic_Speech_Recognition_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx pip install speechbrain\")\n",
        "    ipython.magic(\"sx pip install git+https://github.com/Talent-sprint/ASR_TTS.git\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/ASR_datasets.zip\")\n",
        "    ipython.magic(\"sx unzip ASR_datasets.zip\")\n",
        "    ipython.magic(\"sx git clone https://github.com/Talent-sprint/ASR_TTS\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Levenshtein_Distance_algorithm.py\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us6NdsRFi6_Q"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCRBT4-hqhS8"
      },
      "outputs": [],
      "source": [
        "# The asr and utils contains the paths to the dataset and the pretrained model used\n",
        "from ASR_TTS import asr, utils\n",
        "# The torchaudio package consists of I/O, popular datasets and common audio transformations.\n",
        "import torchaudio\n",
        "import torch\n",
        "from Levenshtein_Distance_algorithm import levenshtein, find_wer_and_cer\n",
        "# Pad a list of variable length Tensors with padding_value\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp_F9DWO379S"
      },
      "source": [
        "**SpeechBrain** is an open-source and all-in-one speech toolkit. It is designed to be simple, extremely flexible, and user-friendly. Also, for various domains we obtain a competitive or state-of-the-art performance using SpeechBrain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmf1KHEN6g32"
      },
      "source": [
        "#### What can we do with SpeechBrain?\n",
        "\n",
        "You can use SpeechBrain for the following types of problems:\n",
        "\n",
        "- **Speech classification** (many-to-one, e.g. speaker-id)\n",
        "- **Speech regression** (speech-to-speech mapping, e.g, speech enhancement)\n",
        "- **Sequence-to-sequence** (speech to speech mapping, e.g., speech recognition)\n",
        "\n",
        "![SpeechBrain-Page-4.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKem2vASoBYd"
      },
      "source": [
        "More precisely, SpeechBrain supports the following tasks:\n",
        "- **Speech Recognition**: Speech-to-text ([see this tutorial](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing))\n",
        "- **Speaker Recognition**: Speaker verification/ID ([see this tutorial](https://colab.research.google.com/drive/1UwisnAjr8nQF3UnrkIJ4abBMAWzVwBMh?usp=sharing)).\n",
        "- **Speaker Diarization**: Detect who spoke when.\n",
        "- **Speech Enhancement**: Noisy to clean speech ([see this tutorial](https://colab.research.google.com/drive/18RyiuKupAhwWX7fh3LCatwQGU5eIS3TR?usp=sharing)).\n",
        "- **Speech Separation**: Separate overlapped speech ([see this tutorial](https://colab.research.google.com/drive/1YxsMW1KNqP1YihNUcfrjy0zUp7FhNNhN?usp=sharing)).\n",
        "- **Spoken Language Understanding**: Speech to intent/slots.\n",
        "- **Multi-microphone processing**: Combining input signals ([see this tutorial](https://colab.research.google.com/drive/1UVoYDUiIrwMpBTghQPbA6rC1mc9IBzi6?usp=sharing)).\n",
        "\n",
        "**Note:** Refer the following [link](https://speechbrain.github.io/) to know more details about SpeechBrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nHAAcVTxWHH"
      },
      "outputs": [],
      "source": [
        "# Evaluate ASR model with different datasets\n",
        "asr.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub1yDGZh5DQx"
      },
      "source": [
        "From the above table observe, that the\n",
        "\n",
        "**librispeech-clean:** refers to the following dataset path `ASR_datasets/test_sets/test-clean.tar.gz`\n",
        "\n",
        "**librispeech-other:** refers to the following dataset path `ASR_datasets/test_sets/test-other.tar.gz`\n",
        "\n",
        "**commonvoice-clean:** refers to the following dataset path `ASR_datasets/test_sets/cv-test.zip`\n",
        "\n",
        "**whisper-spire:**  refers to the following dataset path `ASR_datasets/test_sets/WSpire-test.zip`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uJChjiMaLpA"
      },
      "source": [
        "#### Dataset Structure\n",
        "\n",
        "**Data Instances**\n",
        "\n",
        "A typical data point comprises the path to the audio **file**, usually called file and its transcription, called **text**. Some additional information about the speaker and the passage is provided which contains the transcription.\n",
        "\n",
        "**Librispeech Data Fields**\n",
        "\n",
        "For one of the speech sample in the testset\n",
        "\n",
        "    {'chapter_id': 5040,\n",
        "    'file':'/content/librispeech-other/LibriSpeech/test-other/3080/5040/3080-5040-0005.flac'',\n",
        "    'id': '3080-5040-0005',\n",
        "    'speaker_id': 3080,\n",
        "    'text': 'THE TRUTH IS I COULD NOT ENDURE TO BE MISSUS BRIDE IN A PUBLIC WEDDING TO BE MADE THE HAPPIEST PERSON ON EARTH'}\n",
        "\n",
        "    file: A path to the downloaded audio file in .flac format which includes the Unique id of the speaker\n",
        "\n",
        "    text: The transcription of the audio file.\n",
        "\n",
        "    id: Unique id of the data sample.\n",
        "\n",
        "    speaker_id: Unique id of the speaker. The same speaker id can be found for multiple data samples.\n",
        "\n",
        "    chapter_id: Id of the audiobook chapter which includes the transcription.\n",
        "\n",
        "\n",
        "The **commonvoice-clean**  dataset is in the format of .mp3 files and the corresponding transcriptions are present in `transcripts.csv` file.\n",
        "\n",
        "**Commonvoice-clean Data Fields**\n",
        "\n",
        "The `transcripts.csv` contains following fields\n",
        "\n",
        "    filename: A path to the downloaded audio file in .mp3 format which includes the Unique id of the speaker\n",
        "    text: The transcription of the audio file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80SR58121B0w"
      },
      "outputs": [],
      "source": [
        "# Select the dataset name to be tested\n",
        "dataset_name = 'librispeech-other'\n",
        "unpacked_path = utils.unpack_from_drive(dataset_name)\n",
        "print(unpacked_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLhtcI7-pJu6"
      },
      "outputs": [],
      "source": [
        "# The selected test set contains the below files related to the audio books\n",
        "!ls $unpacked_path/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8K-6ckUpJ0I"
      },
      "outputs": [],
      "source": [
        "# Get the audio and corresponding text files from the dataset\n",
        "audio_text_dict = utils.parse_files(unpacked_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWSyuESN512k"
      },
      "outputs": [],
      "source": [
        "# Print the dictionary (key:value)\n",
        "# key: audio file and value: transcription\n",
        "print(audio_text_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzNb1cZzvxUh"
      },
      "source": [
        "### Encoder-Decoder with attention\n",
        "One of the popular way to approach speech recognition is via an encoder-decoder architecture.\n",
        "The **encoder** is fed by a sequence of speech features (or from the raw samples directly) and generates a sequence of states h.\n",
        "The **decoder** is fed by the last hidden state and outputs the N output tokens.\n",
        "The decoder is often autoregressive (i.e., the previous output is fed back into the input). Decoding is stopped when the end-of-sentence (eos) token is predicted.\n",
        "The encoder and decoders can be based on any neural architectures (e.g., RNN, CNN, Transformers, or a combination of them).\n",
        "\n",
        "The **attention** creates dynamic connections across encoder and decoder states. SpeechBrain supports different types of attention such as *content* or *location-aware* for RNN-based systems and *key-value*-based for Transformers.\n",
        "To improve the convergence often a CTC loss is applied on the top of the encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuZ9OY_tYopV"
      },
      "source": [
        "#### RNN Language Model\n",
        "\n",
        "Applying deep learning for end-to-end ASR allows the model to learn from the data instead of relying on heavily engineered features, allowing the models to learn from the data directly.\n",
        "\n",
        "\n",
        "**Pipeline description**\n",
        "\n",
        "This ASR system is composed with 3 different but linked blocks:\n",
        "\n",
        "* Tokenizer (unigram) that transforms words into subword units and trained with the train transcriptions of LibriSpeech.\n",
        "    \n",
        "* Neural language model (Recurrent Neural Net Language Model (RNNLM) is a type of neural network language models which contains the RNNs in the network. Since an RNN can deal with the variable length inputs, it is suitable for modeling the sequential data such as sentences in natural language trained on the full 10M words dataset.\n",
        "    \n",
        "* Acoustic model (CRDNN (Convolutional Recurrent Deep Neural Networks) + CTC (Connectionist Temporal Classification)/Attention). The CRDNN architecture is made of N blocks of convolutional neural networks with normalization and pooling on the frequency domain. Then, a bidirectional LSTM is connected to a final DNN to obtain the final acoustic representation that is given to the CTC and attention decoders.\n",
        "\n",
        "* **Gated recurrent units (GRUs)** are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets. For more details refer to the following [link](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n",
        "\n",
        "* The Loss function used is **Negative Log-Likelihood Loss (NLL):**\n",
        "$ loss(x,y) = -log(y)$.\n",
        "\n",
        "* NLL loss maximizes the overall probability of the data. It penalizes the model when it predicts the correct class with smaller probabilities and incentivizes when the prediction is made with higher probability. The logrithm does the penalizing part here. Smaller the probabilities, higher will be its logrithm. The negative sign is used here because the probabilities lie in the range [0, 1] and the logrithms of values in this range is negative. So it makes the loss value to be positive.\n",
        "\n",
        "* Learning rate scheduler seek to adjust the learning rate during training by reducing the learning rate according to a pre-defined scheduler.\n",
        "\n",
        "**Note:** Refer to the following [link](https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706) for more details of CRDNN and CTC\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/RNNLM.png\" width=800px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8J9fDH2ZShe"
      },
      "source": [
        "#### Transformer for LibriSpeech (with Transformer Language Model)\n",
        "\n",
        "**Pipeline description**\n",
        "\n",
        "This ASR system is composed of 3 different linked blocks:\n",
        "\n",
        "* Tokenizer (unigram) that transforms words into subword units and trained with the train transcriptions of LibriSpeech.\n",
        "    \n",
        "* Neural language model (Transformer LM) trained on the full 10M words dataset.\n",
        "    \n",
        "* Acoustic model made of a transformer encoder and a joint decoder with CTC (Connectionist Temporal Classification) + transformer. Hence, the decoding also incorporates the CTC probabilities.\n",
        "\n",
        "* Connectionist Temporal Classification (CTC Loss), is simply a loss function that is used to train Neural Networks, like Cross-Entropy and so on. It is used at problems, where having aligned data is an issue, like Speech Recognition. By using CTC, there is no need for aligned data. That is because it can assign a probability for any label, given an input. This means it only requires an audio file as input and a corresponding transcription.  But how can CTC assign a probability for any label, just given an input? CTC is \"alignment-free“. It works by summing over the probability of all possible alignments between the input and the label.\n",
        "\n",
        "\n",
        "The Speech-Transformer transforms the speech feature sequence to the corresponding character sequence. The feature sequence which is longer than the output character sequence is constructed from 2-dimensional spectrograms with time and frequency dimensions. More specifically, CNNs are used to exploit the structure locality of spectrograms and mitigate the length mismatch by striding along time.\n",
        "\n",
        "In the Speech Transformer, 2D attention is used in order to attend at both the frequency and the time dimensions. The queries, keys, and values are extracted from convolutional neural networks and fed to the two self-attention modules.\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Speech_Transformer.png\" width=600px/>\n",
        "</center>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zHdMpe3ZWmm"
      },
      "source": [
        "#### Wav2vec 2.0 with CTC/Attention trained on CommonVoice English (No LM)\n",
        "\n",
        "**Pipeline description**\n",
        "\n",
        "\n",
        "This ASR system is composed of 2 different but linked blocks:\n",
        "\n",
        "*   Tokenizer (unigram) that transforms words into subword units and trained with the train transcriptions (train.tsv) of CommonVoice (EN).\n",
        "*   Acoustic model (wav2vec2.0 + CTC/Attention). A pretrained wav2vec 2.0 model (wav2vec2-lv60-large) is combined with two DNN layers and finetuned on CommonVoice En. The obtained final acoustic representation is given to the CTC and attention decoders.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Wav2vec.png\" width=800px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        " *   Left: the structure of wav2vec2.0 and corresponding self-training criterion. It contains a stack of convolution layers and self-attention layers;\n",
        "\n",
        " *   Right: two decoding branches that apply wav2vec2.0 to ASR tasks with additional projection or decoder,\n",
        "which is trained with CTC or cross-entropy loss respectively.\n",
        "\n",
        "wav2vec2.0 consists of multiple convolution layers and self-attention layers. This structure is widely\n",
        "used in recent end-to-end ASR models. Convolution layers down-sample speech X and generate more compressed latent representation Z. Specifically, Z represents the\n",
        "raw audio signals X sampled by 16k with a stride of about\n",
        "20ms and a receptive field of 25ms. Self-attention layers\n",
        "build contextualized representations C and capture high level\n",
        "content from input Z. Their strong context dependency modeling ability empowers the model to make the right choices\n",
        "during the following contrastive training given the masked Z.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV0PVyQgIn8N"
      },
      "outputs": [],
      "source": [
        "# Listing the model names and the pretrained model and links used\n",
        "asr.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6pF6ldqC2lH"
      },
      "source": [
        "### Loading the pretrained EncoderDecoderASR\n",
        "\n",
        "From the `speechbrain.pretrained` we are loading a ready-to-use Encoder-Decoder ASR model. The class can be used either to run only the encoder (encode()) to extract features or to run the entire encoder-decoder model (transcribe()) to transcribe speech.\n",
        "\n",
        "For more details refer to the following [link](https://speechbrain.readthedocs.io/en/latest/API/speechbrain.pretrained.interfaces.html#module-speechbrain.pretrained.interfaces) about speechbrain.pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebKVJVpuOxDk"
      },
      "outputs": [],
      "source": [
        "# The EncoderDecoderASR works as as wrapper while passing the 'rnn', 'transformers' and 'wav2vec'\n",
        "from speechbrain.pretrained import EncoderDecoderASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGSMe-Ipf7tj"
      },
      "outputs": [],
      "source": [
        "# Select the model name\n",
        "source, savedir = asr.select_model('rnn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umkgy6MErjBb"
      },
      "outputs": [],
      "source": [
        "# A ready-to-use Encoder-Decoder ASR model\n",
        "# The hyper parameter to pass are - source: The location to use for finding the model\n",
        "# savedir (str or Path) – Where to store the pretraining material.\n",
        "asr_model = EncoderDecoderASR.from_hparams(source=source, savedir=savedir, run_opts={\"device\":\"cuda\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h9exnBO8wo"
      },
      "outputs": [],
      "source": [
        "# Record your own audio and after recording click on stop button\n",
        "sr, y = utils.get_audio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJzg_oG8r7qn"
      },
      "outputs": [],
      "source": [
        "# Print the Sampling Rate\n",
        "print(sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1haZO6TEYYOj"
      },
      "outputs": [],
      "source": [
        "# Save the recorded audio samples\n",
        "utils.save_audio(y, sr, path='./temp.flac')\n",
        "\n",
        "# Transcribes the given audio file into a sequence of words\n",
        "# Parameters: path (str) – Path to audio file which to transcribe.\n",
        "# Returns: The audiofile transcription produced by the ASR model\n",
        "asr_model.transcribe_file('./temp.flac')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5YJjQ9jbAe6"
      },
      "source": [
        "### Evaluate different ASR models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-yI2lWhwb7k"
      },
      "outputs": [],
      "source": [
        "# Define the batch size\n",
        "batch_size = 1\n",
        "\n",
        "# Limit the total number of files to be considered\n",
        "limit_samples = 10\n",
        "\n",
        "# Select one of the model name for testing\n",
        "# ASR modelnames: 'rnn', 'transformer', 'wav2vec'\n",
        "# Check the performance of different ASR models\n",
        "modelname = 'rnn'\n",
        "\n",
        "source, savedir = asr.select_model(modelname)\n",
        "asr_model = EncoderDecoderASR.from_hparams(source=source, savedir=savedir, run_opts={\"device\":\"cuda\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiTXRI9P1jYn"
      },
      "source": [
        "#### The Levenshtein Distance Algorithm\n",
        "\n",
        "The Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions, or substitutions) required to change one word into the other. It is named after Vladimir Levenshtein, who discovered this equation in 1965.\n",
        "\n",
        "Levenshtein distance may also be referred to as **edit distance**, although it may also denote a larger family of distance metrics. It is closely related to pairwise string alignments.\n",
        "\n",
        "#### Definition\n",
        "\n",
        "\n",
        "Mathematically, the Levenshtein distance between two strings, $a$ and $b$ (of length $|a|$ and $|b|$ respectively), is given by lev $(a,b)$ where:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Levenshtein_distance.png\" width=600px/>\n",
        "</center>\n",
        "\n",
        "where the $tail$ of some string $x$ is a string of all but the first character of $x$ and $|x|$ is the $n$th character of the string $x$, starting with character 0.\n",
        "\n",
        "Note that the first element in the minimum corresponds to deletion (from a to b), the second to insertion and the third to replacement.\n",
        "\n",
        "For more details refer to the following [link](https://en.wikipedia.org/wiki/Levenshtein_distance) and\n",
        "\n",
        "To understand an example refer to the following [link](https://medium.com/@ethannam/understanding-the-levenshtein-distance-equation-for-beginners-c4285a5604f0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYGQB5wKIaHx"
      },
      "source": [
        "#### Metrics — Word Error Rate (WER)\n",
        "\n",
        "After training our network, we must evaluate how well it performs. A commonly used metric for Speech-to-Text problems is the Word Error Rate (and Character Error Rate). It compares the predicted output and the target transcript, word by word (or character by character) to figure out the number of differences between them.\n",
        "\n",
        "A difference could be a word that is present in the transcript but missing from the prediction (counted as a Deletion), a word that is not in the transcript but has been added into the prediction (an Insertion), or a word that is altered between the prediction and the transcript (a Substitution).\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/WER.png\" width=600px/>\n",
        "</center>\n",
        "$\\hspace{6cm} \\text {Count the Insertions, Deletions, and Substitutions between the Transcript and the Prediction}$\n",
        "<br><br>\n",
        "\n",
        "The metric used is the percent of differences relative to the total number of words.\n",
        "\n",
        "\n",
        "\n",
        "Word\\_Error\\_Rate = $\\frac{I \\ + D \\ + S}{N}$ = $\\frac{1 \\ + 1 \\ + 1}{6}$ = $0.5$\n",
        "\n",
        "Where,\n",
        "\n",
        "*   'I' is the number of word insertions,\n",
        "*   'D' is the number of word deletions,\n",
        "*   'S' is the number of word substitutions, and\n",
        "*   'N' is the total number of words in the target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt0UE9Hs8ZYO"
      },
      "outputs": [],
      "source": [
        "# Here we evaluate/test the ASR model on the batches of speech files\n",
        "def batch_evaluate(dict, datasetname, modelname):\n",
        "\n",
        "    # Total length of the audio text samples\n",
        "    total_samples = len(dict)\n",
        "    # print(list(dict.keys()))\n",
        "\n",
        "    total_iter =  total_samples if limit_samples == None else limit_samples\n",
        "    print(f'{total_samples} audio-text pairs found in test set')\n",
        "    print(f'Evaluating {total_iter} pairs')\n",
        "\n",
        "    cer, wer = [], []\n",
        "    while(len(cer) < limit_samples):\n",
        "\n",
        "        # Audio files (.mp3 or .wav or .flac)\n",
        "        audio_batch = list(dict.keys())[len(cer):len(cer)+batch_size]\n",
        "\n",
        "        # Transcriptions corresponding to the audio batch\n",
        "        text_batch = list(dict.values())[len(cer):len(cer)+batch_size]\n",
        "\n",
        "        sigs, lens = [], []\n",
        "        # Loop through each audio file for the batches of audio samples\n",
        "        for audio_file in audio_batch:\n",
        "            # we use torchaudio.load() to convert the wav files to tensors. torchaudio.load() returns a tuple containing the newly\n",
        "            # created tensor along with the sampling frequency of the audio file (16000kHz)\n",
        "            # It returns Tuple[torch.Tensor, int], an output tensor of size `[C x L]` or `[L x C]` where L is the number\n",
        "            # of audio frames and C is the number of channels. An integer which is the sample rate of the audio.\n",
        "            # It consists of a sequence of numbers, each representing a measurement of the intensity or amplitude of the sound at a particular moment in time.\n",
        "            # The number of such measurements is determined by the sampling rate.\n",
        "            snt, fs = torchaudio.load(audio_file)\n",
        "            sigs.append(snt.squeeze())\n",
        "            lens.append(snt.shape[1])\n",
        "\n",
        "        # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "        # For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise.\n",
        "        # B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence.\n",
        "        # L is length of the sequence. * is any number of trailing dimensions, including none.\n",
        "        # Make all tensor in a batch the same length by padding with zeros\n",
        "        batch = pad_sequence(sigs, batch_first=True, padding_value=0.0)\n",
        "        lens = torch.Tensor(lens) / batch.shape[1]\n",
        "\n",
        "        # transcribe_batch: Transcribes the input audio into a sequence of words\n",
        "        # which returns: list – Each waveform in the batch transcribed.\n",
        "        # tensor – Each predicted token id\n",
        "        preds = asr_model.transcribe_batch(batch, lens)[0]\n",
        "\n",
        "        for i in range(len(audio_batch)):\n",
        "            # Calculate the word error rate (WER) and character error rate (CER)\n",
        "            # The Levenshtein distance is a number that tells you how different two strings are\n",
        "            # The higher the number, the more different the two strings are\n",
        "            cer_val, wer_val, cer_, wer_ = find_wer_and_cer(preds[i], text_batch[i])\n",
        "            cer.append(cer_val)\n",
        "            wer.append(wer_val)\n",
        "    print('\\ndataset:', datasetname)\n",
        "    print('model:', modelname)\n",
        "    print('CER:', sum(cer)/len(cer))\n",
        "    print('WER:', sum(wer)/len(wer))\n",
        "    return cer, wer\n",
        "\n",
        "# Call the above defined function with the audio text samples\n",
        "# 'dataset_name' is the librispeech-other\n",
        "# 'modelname' is the rnn\n",
        "cer, wer = batch_evaluate(audio_text_dict, dataset_name, modelname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5LIM-KsaPQ"
      },
      "source": [
        "**Exercise:** Evalaute different models on the test data and see the performance of word error rate and character error rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvcHptmvyQbq"
      },
      "source": [
        "Above, we saw how to use the pretrained EncoderDecoderASR model from speechbrain and also listed the datasets to test using the ASR models. Now, we will retrain/finetune the ASR model and see the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ain77zK40hIm"
      },
      "source": [
        "### Importing the required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Please run the below code cell and after running it you will get a message to restart the runtime, click on it and run the following code cell to compelte the finetuning part the ASR model and see the performance.**"
      ],
      "metadata": {
        "id": "TopclUxYZVh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHUEA1mWeSB"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "!sudo apt-get install sox\n",
        "!pip install ffmpeg\n",
        "!pip install -U \"torchaudio<0.12.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The asr and utils contains the paths to the dataset and the pretrained model used\n",
        "from ASR_TTS import asr, utils\n",
        "# The torchaudio package consists of I/O, popular datasets and common audio transformations.\n",
        "import torchaudio\n",
        "import torch\n",
        "from Levenshtein_Distance_algorithm import levenshtein, find_wer_and_cer\n",
        "# Pad a list of variable length Tensors with padding_value\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from speechbrain.pretrained import EncoderDecoderASR"
      ],
      "metadata": {
        "id": "_S6COYZRhrOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H6tYmeBy03v"
      },
      "outputs": [],
      "source": [
        "import speechbrain as sb\n",
        "# The shutil module offers a number of high-level operations on files and collections of files\n",
        "import shutil\n",
        "import json\n",
        "import os\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "# 'DynamicItemDataset' Dataset that reads, wrangles, and produces dicts.\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNcL36uw0rKG"
      },
      "source": [
        "### Loading and unpacking the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWAXCqu4y_BB"
      },
      "outputs": [],
      "source": [
        "# Load the commonvoice-clean data\n",
        "dataset_name = 'commonvoice-clean'\n",
        "unpacked_path = utils.unpack_from_drive(dataset_name)\n",
        "audio_text_dict = utils.data_parse_cv(unpacked_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsvICzSF09ID"
      },
      "outputs": [],
      "source": [
        "# Create a folder named 'cv' and copy the .mp3 files to the cv-dev folder\n",
        "shutil.unpack_archive('ASR_datasets/train_set/cv-dev.zip', 'cv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gNi6Hu36ft6"
      },
      "source": [
        "### Formatting the speech files of commonvoice dataset\n",
        "\n",
        "The commonvoice data contains the speech files in `.mp3` format and the `transcript.csv`. Now we have to map the .mp3 to the transcription based on the speech id which returns a the speech files in a dictionary format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL_G5qZT0_XV"
      },
      "outputs": [],
      "source": [
        "def data_parse_cv(path):\n",
        "\n",
        "    # Get the transcript files for the speech samples .mp3\n",
        "    files = get_files(path, '.csv')\n",
        "\n",
        "    # Read the .mp3 files\n",
        "    with open(files[0], 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # Split the .mp3 files and the corresponding transcription\n",
        "    data = [data_ for data_ in data.split('\\n') if len(data_)>0][1:]\n",
        "\n",
        "    # Mapping the the speech samples in a dictionary form as (key:value) pair\n",
        "    mapping = {os.path.join(path, data_.split(',')[0].split('/')[-1]): data_.split(',')[1] for data_ in data}\n",
        "    return mapping\n",
        "\n",
        "def get_files(path, extension):\n",
        "    if isinstance(path, str): path = Path(path).expanduser().resolve()\n",
        "    return [str(n) for n in list(path.rglob(f'*{extension}'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm7TuQXy2Akt"
      },
      "outputs": [],
      "source": [
        "# Call the 'data_parse_cv' function defined above which returns the key:value pair of speech sample and the transcription\n",
        "ddict = data_parse_cv('cv/cv-dev')\n",
        "examples = {}\n",
        "\n",
        "for idx, key in enumerate(ddict):\n",
        "\n",
        "    # Take the .mp3 files which has the transcriptions less than 5 words\n",
        "    if len(ddict[key].split(' ')) < 5:\n",
        "\n",
        "        # Split the 'key' with utterance id from the .mp3 files\n",
        "        utt_id = key.split('-')[-1][:-4]\n",
        "\n",
        "        # Formatting the dictionary with the below attributes which our model expects\n",
        "        examples[utt_id] = {\"file_path\": key,\n",
        "                            \"words\": ddict[key].upper(),\n",
        "                            \"spkID\": f\"speaker_{idx}\",\n",
        "                            \"length\": len(librosa.load(key)[0])\n",
        "                            }\n",
        "# Print the number of speech files in the dataset\n",
        "print(len(examples))\n",
        "\n",
        "# Saving the Formatted data to .json file\n",
        "with open(\"data.json\", \"w\") as f:\n",
        "    json.dump(examples, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGn0VxKRwmIp"
      },
      "source": [
        "The pretrained ASR contains the following fields `asr.ckpt`, `hyperparams.yaml`, `lm.ckpt`, `normalizer.ckpt`, `tokenizer.ckpt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98wIQHRox1Fr"
      },
      "outputs": [],
      "source": [
        "# Loading the pretrained EncoderDecoderASR model and saving it in the 'pretrained_ASR' directory\n",
        "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93uO4GgAybDZ"
      },
      "outputs": [],
      "source": [
        "print(asr_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH6cpRyYb6uN"
      },
      "outputs": [],
      "source": [
        "# Here we update the existing hyperparamters of hyperparams.yaml in the ASR model which is under 'pretrained_ASR'\n",
        "# Model: E2E ASR with attention-based ASR\n",
        "# Encoder: CRDNN (Convolutional Recurrent Neural Network) model\n",
        "# Decoder: GRU + beamsearch + RNNLM\n",
        "# Here we can finetune the feature parameters, model parameters, decoder paramters etc...\n",
        "# Copy the updated hyperparams.yaml to the 'pretrained_ASR' directory\n",
        "!cp ASR_TTS/ASR_TTS/hyperyaml.yaml /content/pretrained_ASR/hyperparams.yaml\n",
        "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\", run_opts={\"device\":\"cuda\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvSQFuJA1Fgk"
      },
      "source": [
        "### Load the json data\n",
        "\n",
        "Here we are loading the speech files from `data.json` and sort the key's by selcting only 395 words, Next we read the audio files which takes the path and provides the signal\n",
        "\n",
        "Refer to the following  [link](https://speechbrain.readthedocs.io/en/latest/API/speechbrain.dataio.dataset.html#speechbrain.dataio.dataset.DynamicItemDataset) for DynamicItemDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9P3vyu6_En6"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('cv'):\n",
        "    shutil.unpack_archive('ASR_datasets/test_set/cv-dev.zip', 'cv')\n",
        "\n",
        "# Define the dataset to load the json data manifest file\n",
        "dataset = DynamicItemDataset.from_json(\"data.json\")\n",
        "\n",
        "# Sort the dataset by the length of the .mp3 files and the number of sentences to be considered as 395\n",
        "# Define datasets sorted by ascending lengths for efficiency\n",
        "dataset = dataset.filtered_sorted(sort_key=\"length\", select_n=395)\n",
        "\n",
        "# Audio Loading from json file which takes the path of the speech sample\n",
        "dataset.add_dynamic_item(sb.dataio.dataio.read_audio, takes=\"file_path\", provides=\"signal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4grMnuVnAPXH"
      },
      "outputs": [],
      "source": [
        "# Define text processing pipeline. We start from the raw text and then\n",
        "# encode it using the tokenizer. The tokens with BOS(begin of sentence) are used for feeding\n",
        "# decoder during training, the tokens with EOS(end of sentence) for computing the cost function.\n",
        "\n",
        "# It takes words and provides the \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
        "@sb.utils.data_pipeline.takes(\"words\")\n",
        "@sb.utils.data_pipeline.provides(\n",
        "        \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\")\n",
        "\n",
        "def text_pipeline(words):\n",
        "    yield words\n",
        "    # Tokenize the words\n",
        "    tokens_list = asr_model.tokenizer.encode_as_ids(words)\n",
        "    yield tokens_list\n",
        "    # Add the index at the begining and ending of the sentences\n",
        "    tokens_bos = torch.LongTensor([asr_model.hparams.bos_index] + (tokens_list))\n",
        "    yield tokens_bos\n",
        "    tokens_eos = torch.LongTensor(tokens_list + [asr_model.hparams.eos_index]) # we use same eos and bos indexes as in pretrained model\n",
        "    yield tokens_eos\n",
        "    # Returns the list of unique token ids for the words\n",
        "    tokens = torch.LongTensor(tokens_list)\n",
        "    yield tokens\n",
        "\n",
        "dataset.add_dynamic_item(text_pipeline)\n",
        "# Set output keys to add into the batch. The batch variable will contain all these fields (e.g, id, signal, wordss,..)\n",
        "dataset.set_output_keys([\"id\", \"signal\", \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"])\n",
        "# dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J70uv0U2Dxvr"
      },
      "source": [
        "### Finetuning the EncoderDecoderASR model\n",
        "\n",
        "#### `on_stage_start`\n",
        "\n",
        "It starts the epoch iteration and prepares for iterating the train data. To adjust the preparation one can override the `on_stage_start` method, which will allow for things like creating containers to store training statistics.\n",
        "\n",
        "#### `on_stage_end`\n",
        "\n",
        "At the end of the training loop, the `on_stage_end` method is called for potential cleanup operations, such as reporting training statistics.\n",
        "\n",
        "\n",
        "One key component of deep learning is iterating the dataset multiple times and performing parameter updates. This process is sometimes called the \"training loop\" and there are usually many stages to this loop. SpeechBrain provides a convenient framework for organizing the training loop, in the form of a class known as the \"Brain\" class, implemented in `speechbrain/core.py`. In each recipe, we sub-class this class and override the methods for which the default implementation doesn't do what is required for that particular recipe.\n",
        "\n",
        "The primary method for this class is the `fit()` method, which takes a set of data and iterates it many times and performs updates to a model. In order to use `fit()`, a minimum of two methods must be defined in the sub-class: `compute_forward()` and `compute_objectives()`. These methods define the computation of the model to generate predictions, as well as the loss terms needed to find a gradient.\n",
        "\n",
        "#### Forward Computations\n",
        "\n",
        "In the Brain class we have to define some important methods such as:\n",
        "\n",
        "**compute_forward**, that specifies all the computations needed to transform the input waveform into the output posterior probabilities)\n",
        "\n",
        "**compute_objective**, which computes the loss function given the labels and the predictions performed by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82tA9HecAqmM"
      },
      "outputs": [],
      "source": [
        "class EncDecFineTune(sb.Brain):\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        # Select the current stage (that can be sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST)\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            # The modules to be passed through our model\n",
        "            for module in [self.modules.enc, self.modules.emb, self.modules.dec, self.modules.seq_lin]:\n",
        "                # Unfreeze the weights of the pretrained model\n",
        "                for p in module.parameters():\n",
        "                    p.requires_grad = True\n",
        "\n",
        "    def on_stage_end(self, stage, a, aa):\n",
        "        self.checkpointer.save_and_keep_only()\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"\n",
        "        Predicts the next word given the previous ones..\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : dict\n",
        "            At training time it returns predicted seq2seq log probabilities.\n",
        "            If needed it also returns the ctc output log probabilities.\n",
        "            At validation/test time, it returns the predicted tokens as well.\n",
        "        \"\"\"\n",
        "\n",
        "        # We first move the batch to the appropriate device.\n",
        "        self.device = 'cuda'\n",
        "        batch = batch.to(self.device)\n",
        "\n",
        "        # batch.signal returns a batch of waveforms [batch, time, channels] or [batch, time] and the relative lengths\n",
        "        wavs, wav_lens = batch.signal\n",
        "\n",
        "        # We just put the batch on the right device and feed the encoded tokens into the model. We feed the tokens with <bos> into the model\n",
        "        # When adding the <bos> token, in fact, we shift all the tokens by one element.\n",
        "        # This way, our input corresponds to the previous token while our model tries to predict the current one\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "\n",
        "        # Extracting the features\n",
        "        feats = self.modules.compute_features(wavs)\n",
        "\n",
        "        # Feature normalization (mean and std)\n",
        "        feats = self.modules.normalize(feats, wav_lens)\n",
        "\n",
        "        # Encode the features with our CRDNN(Convolutional Recurrent Deep Neural Network) encoder\n",
        "        x = self.modules.enc(feats)\n",
        "\n",
        "        # Embed tokens and pass tokens & encoded signal to decoder\n",
        "        e_in = self.modules.emb(tokens_bos)\n",
        "\n",
        "        # we feed our encoded states into an autoregressive attention-based decoder that performs some predictions over the tokens\n",
        "        h, _ = self.modules.dec(e_in, x, wav_lens)\n",
        "\n",
        "        # Linear transformation on the top of the decoder\n",
        "        # Output layer for seq2seq log-probabilities\n",
        "        logits = self.modules.seq_lin(h)\n",
        "\n",
        "        # Find the softmax (for log posteriors computation)\n",
        "        p_seq = self.hparams.log_softmax(logits)\n",
        "        return p_seq, wav_lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"\n",
        "        Predicts the next word given the previous ones..\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : dict\n",
        "            At training time it returns predicted seq2seq log probabilities.\n",
        "            If needed it also returns the ctc output log probabilities.\n",
        "            At validation/test time, it returns the predicted tokens as well.\n",
        "        \"\"\"\n",
        "        # We first move the batch to the appropriate device.\n",
        "\n",
        "        p_seq, wav_lens = predictions\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "        loss = self.hparams.seq_cost(\n",
        "            p_seq, tokens_eos, tokens_eos_lens)\n",
        "        return loss\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\" The fit_batch trains each batch of data (by computing the gradient with the backward method\n",
        "          and the updates with step one). The on_stage_end, is called at the end of each stage\n",
        "          (e.g, at the end of each training epoch) and mainly takes care of statistic management, learning rate annealing, and checkpointing\n",
        "\n",
        "        \"\"\"\n",
        "        # Get the predictions by setting the stage to train\n",
        "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "        # Compute the loss\n",
        "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
        "        # Do the backward pass\n",
        "        loss.backward()\n",
        "        if self.check_gradients(loss):\n",
        "            self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return loss.detach()\n",
        "\n",
        "# Trainer initialization with the modules\n",
        "modules = {\"enc\": asr_model.mods.encoder.model,\n",
        "           \"emb\": asr_model.hparams.emb,\n",
        "           \"dec\": asr_model.hparams.dec,\n",
        "           \"compute_features\": asr_model.mods.encoder.compute_features, # we use the same features\n",
        "           \"normalize\": asr_model.mods.encoder.normalize,\n",
        "           \"seq_lin\": asr_model.hparams.seq_lin}\n",
        "\n",
        "# Set the hyperparameters: Cost function used for training the model is nll_loss\n",
        "# softmax function is used along with the nll_loss\n",
        "# label_smoothing is a regularization method to penalize overconfident predictions and improve generalization and\n",
        "# It has been used in many state-of-the-art models, including image classification, language translation, and speech recognition\n",
        "hparams = {\"seq_cost\": lambda x, y, z: sb.nnet.losses.nll_loss(x, y, z, label_smoothing = 0.1),\n",
        "            \"log_softmax\": sb.nnet.activations.Softmax(apply_log=True), \"device\":'cuda:0'}\n",
        "\n",
        "# Call the function with modeules and the optimizer\n",
        "# This optimizer (SGD) will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference\n",
        "brain = EncDecFineTune(modules, hparams=hparams, opt_class=lambda x: torch.optim.SGD(x, 1e-5), checkpointer=asr_model.hparams.checkpointer)\n",
        "brain.tokenizer = asr_model.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXbIfzS0jigc"
      },
      "source": [
        "### Train the model on commonvoice-clean dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePHezEDoBexm"
      },
      "outputs": [],
      "source": [
        "# Increase no of epochs to see the change in performance\n",
        "num_epochs = 4\n",
        "batch_size = 1\n",
        "brain.fit(range(num_epochs), train_set=dataset,\n",
        "          train_loader_kwargs={\"batch_size\": batch_size, \"drop_last\":True, \"shuffle\": False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RV1_LfkCXUx"
      },
      "outputs": [],
      "source": [
        "# Saving the checkpoint and copying it to the asr.cpt\n",
        "!cp save/*/model.ckpt pretrained_ASR/asr.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utjv0hlXkXea"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we evaluate/test the ASR model on the batches of speech files\n",
        "def batch_evaluate(dict, datasetname, modelname):\n",
        "\n",
        "    # Total length of the audio text samples\n",
        "    total_samples = len(dict)\n",
        "    # print(list(dict.keys()))\n",
        "\n",
        "    total_iter =  total_samples if limit_samples == None else limit_samples\n",
        "    print(f'{total_samples} audio-text pairs found in test set')\n",
        "    print(f'Evaluating {total_iter} pairs')\n",
        "\n",
        "    cer, wer = [], []\n",
        "    while(len(cer) < limit_samples):\n",
        "\n",
        "        # Audio files (.mp3 or .wav or .flac)\n",
        "        audio_batch = list(dict.keys())[len(cer):len(cer)+batch_size]\n",
        "\n",
        "        # Transcriptions corresponding to the audio batch\n",
        "        text_batch = list(dict.values())[len(cer):len(cer)+batch_size]\n",
        "\n",
        "        sigs, lens = [], []\n",
        "        # Loop through each audio file for the batches of audio samples\n",
        "        for audio_file in audio_batch:\n",
        "            # we use torchaudio.load() to convert the wav files to tensors. torchaudio.load() returns a tuple containing the newly\n",
        "            # created tensor along with the sampling frequency of the audio file (16000kHz)\n",
        "            # It returns Tuple[torch.Tensor, int], an output tensor of size `[C x L]` or `[L x C]` where L is the number\n",
        "            # of audio frames and C is the number of channels. An integer which is the sample rate of the audio.\n",
        "            # It consists of a sequence of numbers, each representing a measurement of the intensity or amplitude of the sound at a particular moment in time.\n",
        "            # The number of such measurements is determined by the sampling rate.\n",
        "            snt, fs = torchaudio.load(audio_file)\n",
        "            sigs.append(snt.squeeze())\n",
        "            lens.append(snt.shape[1])\n",
        "\n",
        "        # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "        # For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise.\n",
        "        # B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence.\n",
        "        # L is length of the sequence. * is any number of trailing dimensions, including none.\n",
        "        # Make all tensor in a batch the same length by padding with zeros\n",
        "        batch = pad_sequence(sigs, batch_first=True, padding_value=0.0)\n",
        "        lens = torch.Tensor(lens) / batch.shape[1]\n",
        "\n",
        "        # transcribe_batch: Transcribes the input audio into a sequence of words\n",
        "        # which returns: list – Each waveform in the batch transcribed.\n",
        "        # tensor – Each predicted token id\n",
        "        preds = asr_model.transcribe_batch(batch, lens)[0]\n",
        "\n",
        "        for i in range(len(audio_batch)):\n",
        "            # Calculate the word error rate (WER) and character error rate (CER)\n",
        "            # The Levenshtein distance is a number that tells you how different two strings are\n",
        "            # The higher the number, the more different the two strings are\n",
        "            cer_val, wer_val, cer_, wer_ = find_wer_and_cer(preds[i], text_batch[i])\n",
        "            cer.append(cer_val)\n",
        "            wer.append(wer_val)\n",
        "    print('\\ndataset:', datasetname)\n",
        "    print('model:', modelname)\n",
        "    print('CER:', sum(cer)/len(cer))\n",
        "    print('WER:', sum(wer)/len(wer))\n",
        "    return cer, wer"
      ],
      "metadata": {
        "id": "fLNNTMlnW1P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIAhUcJACbjt"
      },
      "outputs": [],
      "source": [
        "# Test the model on commonvoice-clean with the above trained EncoderDecoderASR with above saved checkpoints\n",
        "limit_samples = 100\n",
        "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"./pretrained_ASR\",run_opts={\"device\":\"cuda\"})\n",
        "# Evaluate the model on the testset\n",
        "cer, wer = batch_evaluate(audio_text_dict, dataset_name, 'rnn')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Please run the setup cells again and answer the graded questions section to submit the notebook."
      ],
      "metadata": {
        "id": "kJ2ZIUh-ySzv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statements about Attention and answer Q1.\n",
        "\n",
        "A. Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, independent of the query.\n",
        "\n",
        "B.  Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query.\n",
        "\n",
        "C. Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, independent of the weights.\n",
        "\n",
        "D. Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NHDYadgG-RHl"
      },
      "outputs": [],
      "source": [
        "#@title Q.1. Which of the following statements is TRUE for attention?\n",
        "Answer1 = \"\" #@param [\"\",\"Only A\",\"Only B\",\"Only D\",\"Both A and C\",\"Both C and D\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZFB-LkeKSjL"
      },
      "source": [
        "#### Consider the following statements about Connectionist temporal classification (CTC) and answer Q2.\n",
        "\n",
        "A. Connectionist temporal classification was introduced to provide a method of training RNNs to “label unsegmented sequences indirectly,”\n",
        "\n",
        "B. Connectionist temporal classification was introduced to provide a method of training RNNs to “label segmented sequences directly,”\n",
        "\n",
        "C. Connectionist temporal classification was introduced to provide a method of training RNNs to “label segmented sequences indirectly,”\n",
        "\n",
        "D. Connectionist temporal classification was introduced to provide a method of training RNNs to “label unsegmented sequences directly,”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Qu3sufDD7pu1"
      },
      "outputs": [],
      "source": [
        "#@title Q.2. Which of the following statements is TRUE for Connectionist Temporal Classification?\n",
        "Answer2 = \"\" #@param [\"\",\"Only A\",\"Only C\",\"Only D\",\"Both A and C\",\"Both B and D\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}